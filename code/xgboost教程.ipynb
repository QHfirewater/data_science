{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from  sklearn.model_selection import train_test_split,GridSearchCV,KFold\n",
    "from sklearn.metrics import confusion_matrix,mean_squared_error\n",
    "from sklearn.datasets import load_iris,load_digits,load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(31337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二分类混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= digits['data']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = digits['target']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2,shuffle=True,random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:13:29] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[84  0  0  0  0  2  0  0  0  0]\n",
      " [ 2 86  0  0  0  0  0  0  0  2]\n",
      " [ 1  1 75  3  0  0  1  0  1  1]\n",
      " [ 0  0  1 90  0  2  0  1  2  2]\n",
      " [ 0  0  0  0 84  0  0  0  1  1]\n",
      " [ 0  0  0  0  2 94  0  0  1  2]\n",
      " [ 0  0  2  0  1  0 86  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 93  0  1]\n",
      " [ 0  1  1  0  0  1  0  0 76  2]\n",
      " [ 1  0  0  0  0  2  0  0  4 86]]\n",
      "[18:13:30] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[87  0  0  0  2  0  0  0  3  0]\n",
      " [ 0 90  0  1  0  0  0  0  0  1]\n",
      " [ 1  1 91  0  0  0  0  0  1  0]\n",
      " [ 0  1  1 80  0  1  0  1  1  0]\n",
      " [ 3  1  0  0 88  0  0  1  1  1]\n",
      " [ 0  0  0  0  0 81  0  0  0  2]\n",
      " [ 0  0  0  0  0  0 90  0  2  0]\n",
      " [ 0  0  0  0  1  0  0 83  1  0]\n",
      " [ 0  6  0  2  0  1  2  2 78  2]\n",
      " [ 0  0  0  1  0  0  0  2  0 84]]\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in kf.split(x):\n",
    "    xgb_model = xgb.XGBClassifier().fit(x[train_index],y[train_index])\n",
    "    pred = xgb_model.predict(x[test_index])\n",
    "    actual = y[test_index]\n",
    "    print(confusion_matrix(actual,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:19] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[29  0  0]\n",
      " [ 0 24  1]\n",
      " [ 0  1 20]]\n",
      "[18:16:19] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[21  0  0]\n",
      " [ 0 21  4]\n",
      " [ 0  2 27]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "D:\\software\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "y = iris['target']\n",
    "x = iris['data']\n",
    "kf = KFold(2,shuffle=True,random_state=rng)\n",
    "for train_index,test_index in kf.split(x):\n",
    "    xgb_model = xgb.XGBClassifier().fit(x[train_index],y[train_index])\n",
    "    pred = xgb_model.predict(x[test_index])\n",
    "    actual = y[test_index]\n",
    "    print(confusion_matrix(actual,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.262105023328399\n",
      "10.144190394517386\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "y = boston['target']\n",
    "x = boston['data']\n",
    "kf = KFold(2,shuffle=True,random_state=rng)\n",
    "for train_index,test_index in kf.split(x):\n",
    "    xgb_model = xgb.XGBRegressor().fit(x[train_index],y[train_index])\n",
    "    pred = xgb_model.predict(x[test_index])\n",
    "    actual = y[test_index]\n",
    "    print(mean_squared_error(actual,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boston['target']\n",
    "x  = boston['data']\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "para_dict = {'max_depth':[2,4,6],\n",
    "             'n_estimators':[50,100,200]}\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None,\n",
       "                                    enable_categorical=False, gamma=None,\n",
       "                                    gpu_id=None, importance_type=None,\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, max_delta_step=None,\n",
       "                                    max_depth=None, min_child_weight=None,\n",
       "                                    missing=nan, monotone_constraints=None,\n",
       "                                    n_estimators=100, n_jobs=None,\n",
       "                                    num_parallel_tree=None, predictor=None,\n",
       "                                    random_state=None, reg_alpha=None,\n",
       "                                    reg_lambda=None, scale_pos_weight=None,\n",
       "                                    subsample=None, tree_method=None,\n",
       "                                    validate_parameters=None, verbosity=None),\n",
       "             param_grid={'max_depth': [2, 4, 6],\n",
       "                         'n_estimators': [50, 100, 200]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(xgb_model,para_dict,verbose=1)\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2, 'n_estimators': 100}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        module\n",
      "\u001b[1;31mString form:\u001b[0m <module 'xgboost' from 'D:\\\\software\\\\anaconda\\\\lib\\\\site-packages\\\\xgboost\\\\__init__.py'>\n",
      "\u001b[1;31mFile:\u001b[0m        d:\\software\\anaconda\\lib\\site-packages\\xgboost\\__init__.py\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "XGBoost: eXtreme Gradient Boosting library.\n",
      "\n",
      "Contributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'reg:squarederror'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    n_estimators : int\n",
      "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "        rounds.\n",
      "\n",
      "    max_depth :  Optional[int]\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : Optional[float]\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    verbosity : Optional[int]\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "    objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    booster: Optional[str]\n",
      "        Specify which booster to use: gbtree, gblinear or dart.\n",
      "    tree_method: Optional[str]\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter\n",
      "        is set to default, XGBoost will choose the most conservative option\n",
      "        available.  It's recommended to study this option from the parameters\n",
      "        document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "    n_jobs : Optional[int]\n",
      "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "        balance the threads.  Creating thread contention will significantly slow down both\n",
      "        algorithms.\n",
      "    gamma : Optional[float]\n",
      "        Minimum loss reduction required to make a further partition on a leaf\n",
      "        node of the tree.\n",
      "    min_child_weight : Optional[float]\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : Optional[float]\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : Optional[float]\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : Optional[float]\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : Optional[float]\n",
      "        Subsample ratio of columns for each level.\n",
      "    colsample_bynode : Optional[float]\n",
      "        Subsample ratio of columns for each split.\n",
      "    reg_alpha : Optional[float]\n",
      "        L1 regularization term on weights (xgb's alpha).\n",
      "    reg_lambda : Optional[float]\n",
      "        L2 regularization term on weights (xgb's lambda).\n",
      "    scale_pos_weight : Optional[float]\n",
      "        Balancing of positive and negative weights.\n",
      "    base_score : Optional[float]\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float, default np.nan\n",
      "        Value in the data which needs to be present as a missing value.\n",
      "    num_parallel_tree: Optional[int]\n",
      "        Used for boosting random forest.\n",
      "    monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "        Constraint of variable monotonicity.  See tutorial for more\n",
      "        information.\n",
      "    interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "        [2, 3, 4]], where each inner list is a group of indices of features\n",
      "        that are allowed to interact with each other.  See tutorial for more\n",
      "        information\n",
      "    importance_type: Optional[str]\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "\n",
      "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "          \"total_cover\".\n",
      "        * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "          without bias.\n",
      "\n",
      "    gpu_id : Optional[int]\n",
      "        Device ordinal.\n",
      "    validate_parameters : Optional[bool]\n",
      "        Give warnings for unknown parameter.\n",
      "    predictor : Optional[str]\n",
      "        Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "        gpu_predictor].\n",
      "    enable_categorical : bool\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "        Experimental support for categorical data.  Do not set to true unless you are\n",
      "        interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "\n",
      "    kwargs : dict, optional\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "        parameters can be found here:\n",
      "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n",
      "        .. note::  Custom objective function\n",
      "\n",
      "            A custom objective function can be provided for the ``objective``\n",
      "            parameter. In this case, it should have the signature\n",
      "            ``objective(y_true, y_pred) -> grad, hess``:\n",
      "\n",
      "            y_true: array_like of shape [n_samples]\n",
      "                The target values\n",
      "            y_pred: array_like of shape [n_samples]\n",
      "                The predicted values\n",
      "\n",
      "            grad: array_like of shape [n_samples]\n",
      "                The value of the gradient for each sample point.\n",
      "            hess: array_like of shape [n_samples]\n",
      "                The value of the second derivative for each sample point\n",
      "\u001b[1;31mFile:\u001b[0m           d:\\software\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     XGBRFRegressor\n"
     ]
    }
   ],
   "source": [
    "xgb.XGBRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.98544\n",
      "[1]\tvalidation_0-auc:0.99122\n",
      "[2]\tvalidation_0-auc:0.99268\n",
      "[3]\tvalidation_0-auc:0.99368\n",
      "[4]\tvalidation_0-auc:0.99414\n",
      "[5]\tvalidation_0-auc:0.99495\n",
      "[6]\tvalidation_0-auc:0.99569\n",
      "[7]\tvalidation_0-auc:0.99611\n",
      "[8]\tvalidation_0-auc:0.99650\n",
      "[9]\tvalidation_0-auc:0.99684\n",
      "[10]\tvalidation_0-auc:0.99716\n",
      "[11]\tvalidation_0-auc:0.99746\n",
      "[12]\tvalidation_0-auc:0.99752\n",
      "[13]\tvalidation_0-auc:0.99768\n",
      "[14]\tvalidation_0-auc:0.99794\n",
      "[15]\tvalidation_0-auc:0.99808\n",
      "[16]\tvalidation_0-auc:0.99815\n",
      "[17]\tvalidation_0-auc:0.99828\n",
      "[18]\tvalidation_0-auc:0.99835\n",
      "[19]\tvalidation_0-auc:0.99845\n",
      "[20]\tvalidation_0-auc:0.99854\n",
      "[21]\tvalidation_0-auc:0.99855\n",
      "[22]\tvalidation_0-auc:0.99868\n",
      "[23]\tvalidation_0-auc:0.99869\n",
      "[24]\tvalidation_0-auc:0.99877\n",
      "[25]\tvalidation_0-auc:0.99877\n",
      "[26]\tvalidation_0-auc:0.99881\n",
      "[27]\tvalidation_0-auc:0.99886\n",
      "[28]\tvalidation_0-auc:0.99885\n",
      "[29]\tvalidation_0-auc:0.99891\n",
      "[30]\tvalidation_0-auc:0.99891\n",
      "[31]\tvalidation_0-auc:0.99891\n",
      "[32]\tvalidation_0-auc:0.99891\n",
      "[33]\tvalidation_0-auc:0.99891\n",
      "[34]\tvalidation_0-auc:0.99892\n",
      "[35]\tvalidation_0-auc:0.99892\n",
      "[36]\tvalidation_0-auc:0.99897\n",
      "[37]\tvalidation_0-auc:0.99897\n",
      "[38]\tvalidation_0-auc:0.99897\n",
      "[39]\tvalidation_0-auc:0.99899\n",
      "[40]\tvalidation_0-auc:0.99901\n",
      "[41]\tvalidation_0-auc:0.99903\n",
      "[42]\tvalidation_0-auc:0.99901\n",
      "[43]\tvalidation_0-auc:0.99902\n",
      "[44]\tvalidation_0-auc:0.99902\n",
      "[45]\tvalidation_0-auc:0.99903\n",
      "[46]\tvalidation_0-auc:0.99902\n",
      "[47]\tvalidation_0-auc:0.99904\n",
      "[48]\tvalidation_0-auc:0.99904\n",
      "[49]\tvalidation_0-auc:0.99902\n",
      "[50]\tvalidation_0-auc:0.99904\n",
      "[51]\tvalidation_0-auc:0.99903\n",
      "[52]\tvalidation_0-auc:0.99905\n",
      "[53]\tvalidation_0-auc:0.99904\n",
      "[54]\tvalidation_0-auc:0.99906\n",
      "[55]\tvalidation_0-auc:0.99904\n",
      "[56]\tvalidation_0-auc:0.99906\n",
      "[57]\tvalidation_0-auc:0.99905\n",
      "[58]\tvalidation_0-auc:0.99904\n",
      "[59]\tvalidation_0-auc:0.99904\n",
      "[60]\tvalidation_0-auc:0.99905\n",
      "[61]\tvalidation_0-auc:0.99904\n",
      "[62]\tvalidation_0-auc:0.99903\n",
      "[63]\tvalidation_0-auc:0.99905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = digits['data']\n",
    "y = digits['target']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)\n",
    "clf = xgb.XGBClassifier(use_label_encoder=False)\n",
    "clf.fit(x_train,y_train,early_stopping_rounds=10,eval_metric='auc',eval_set=[(x_test,y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c72f12f90170356fe3abc9748a888f8a0f253bf3dee690070f84c0db82f6a8c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
